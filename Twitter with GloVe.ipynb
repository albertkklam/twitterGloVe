{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe introduction\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\n",
    "\n",
    "More information can be found here: http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We will use word vectors that have been trained on the Twitter dataset (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors). This file can be downloaded from the GloVe website. Extract the .txt file and move it to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset\n",
    "\n",
    "Let's read in the data and do some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193514, 200)\n"
     ]
    }
   ],
   "source": [
    "with open(filename,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    numWords = len(lines)\n",
    "    numDimensions = len(lines[200].split(' ')[1:])\n",
    "    print(numWords, numDimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's slightly less words in the vocabulary than claimed (1,193,514 words vs. 1,200,000) and each word is mapped to a 200 dimensional vector. Let's have a look at what a word vector looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please 0.079204 0.38973 -0.15059 -0.010345 -0.43449 -1.0396 1.142 -0.12891 0.021345 -0.31301 0.67416 0.020708 -0.21758 -0.25822 -0.087623 -0.21197 0.19887 -0.18434 0.11543 -0.045039 -0.21852 -0.4629 -0.40147 0.88832 -0.28331 0.15793 0.43682 0.62241 0.29734 0.025521 0.04076 0.42191 -0.17571 0.38485 -0.222 -0.12087 0.53335 0.60102 -0.14619 -0.2134 0.33717 -0.46093 -0.31229 0.0040756 -0.11045 -0.26965 -0.64615 -0.66332 0.39245 0.10454 0.073493 0.54851 0.36091 -1.1031 0.25083 0.06513 0.046064 0.56705 -0.072345 -0.19426 0.17681 -0.13486 0.33334 -0.18167 0.11279 0.42252 -0.11612 -0.10706 0.1187 -0.044723 0.053748 0.064657 0.12535 0.04816 -0.29935 -0.10651 -0.29289 -0.18884 -0.4127 0.32664 -0.22715 0.67269 0.41074 -0.35499 0.38288 0.083866 -0.76714 0.29737 -0.27832 0.2076 0.015894 -0.074241 0.040225 0.46588 -0.73723 -0.18881 0.062039 0.27367 0.12206 -0.3957 -0.21934 -0.065962 -0.19748 -0.18956 0.027889 -0.126 0.037872 0.54629 0.37619 -0.26709 -0.27878 -0.12053 -0.66942 0.064615 1.2675 0.057054 -0.27236 -0.095923 0.37097 0.0404 -0.48124 0.068381 0.11831 -0.029161 0.69429 -0.15623 -0.51732 -0.10252 -0.029082 1.1329 0.25229 -0.093178 0.59033 -0.15265 0.59541 -0.046142 0.31834 0.52373 0.041039 -0.1108 -0.13369 -0.23856 -0.046343 0.12303 -0.40135 0.11632 0.2686 0.2048 -0.2025 -0.29742 -0.10665 -0.0082937 -5.2843 0.58139 -0.49934 0.21979 -0.48109 -0.34275 0.22535 0.27961 0.57855 -0.86456 -0.11685 -0.051452 -0.12701 -0.44353 0.49309 0.24619 -0.49139 0.58254 0.15034 -0.47087 0.20892 -0.6286 -0.27853 -0.43544 0.19348 -0.29692 -0.34797 -0.53869 0.22205 -0.52296 0.55458 0.43955 0.013287 -0.73236 -0.52186 0.070435 0.24036 0.18332 0.035254 0.31854 -0.25702 -0.26639 -0.39825 -0.57111 -0.34293 -0.16897 0.4296 0.022703\n",
      "\n",
      "via -0.081437 0.069476 0.23265 -0.40076 -0.19034 -0.49862 -0.73485 -0.58641 0.54637 0.33817 0.10446 0.2993 -0.50904 0.15863 -0.1678 -0.11511 -0.54687 -0.085679 0.60643 -0.16164 -0.27374 -0.32401 -0.52601 -0.23146 0.059834 -0.52935 -0.42386 0.098854 -0.18035 0.13567 0.15574 -0.29407 -0.033457 -0.32343 1.0269 -0.2487 0.27185 0.0097102 -0.13825 -0.13629 -1.1641 0.38467 -0.15126 0.0025759 0.19337 -0.093839 -0.25593 -0.051334 -0.58863 -0.58854 0.23793 -0.58243 0.042192 0.60617 0.29011 0.081879 0.53839 -0.37073 0.37057 0.1838 -0.01204 -0.11125 0.19795 0.74184 0.011207 -0.16587 0.35526 0.10512 0.18121 0.53045 0.47704 -0.10744 0.53014 -0.1772 -0.61374 -0.25991 -0.50574 -0.16052 -0.25734 0.03218 0.082489 0.53998 1.1106 -0.21224 0.37242 -0.89264 0.31962 0.40549 0.21223 -0.25742 -0.087136 -0.28339 0.25084 -0.092796 -0.42794 0.2892 -0.14682 0.16072 -0.31523 -0.14671 0.4218 0.52531 0.46894 0.039444 0.43839 0.43707 0.027612 -0.057925 -0.030428 -0.51068 0.42887 0.37742 -0.21719 0.63096 0.70359 0.56866 -0.10866 -0.71073 -0.29489 0.46609 0.58452 0.26705 0.242 -0.54905 -0.097825 -0.31057 -0.5359 0.45974 0.27342 0.49491 0.46291 0.24952 -0.26941 0.19397 0.46285 0.16044 0.093137 0.55253 0.14519 -0.24617 0.14566 -0.17531 -0.079643 -0.21696 0.14657 0.071494 -0.39468 -0.18362 -0.0038123 0.45462 -0.44943 -0.023135 -4.0527 0.12995 0.43728 -0.51105 0.14058 0.14181 0.0027643 0.3928 -0.17767 0.36714 0.26078 -0.0068611 0.49727 -0.56449 -0.33131 0.075125 -0.61215 0.19659 0.30055 0.49532 0.21603 0.16426 -0.064625 0.26461 0.75414 -0.22029 -0.10808 0.3196 0.36554 -0.27764 -0.083928 0.20858 -0.33401 -0.138 -0.18531 -0.4884 0.35663 0.088559 0.09485 -0.10018 0.1194 0.19082 0.1525 0.78007 -0.3482 0.2529 -0.27805 0.32401\n",
      "\n",
      "much -0.22675 0.72306 0.030062 0.56328 -0.21531 0.23392 0.70151 0.067725 -0.46529 -0.45717 0.16344 -0.25265 -0.87955 -0.16559 -0.067855 0.11523 0.11271 0.11077 -0.42187 -0.16591 0.066559 -0.40993 0.51975 0.36871 0.055161 1.2822 0.19334 0.51311 0.32885 -0.20996 -0.12559 0.0090897 0.25718 0.41892 -0.27265 -0.02081 -0.23258 0.17777 -0.51448 0.12906 0.49615 -0.075301 0.30338 -0.34572 -0.13291 -0.19647 0.33912 -0.49893 0.10305 0.019522 0.15939 -0.087156 0.047154 -0.30672 -0.09427 0.025437 0.094385 0.18369 -0.45566 -0.28899 -0.54054 -0.45882 -0.61685 -0.2597 -0.0030352 0.13973 -0.54018 -0.68448 0.042326 -0.22023 -0.051847 -0.23852 0.083819 0.08203 -0.1727 -0.037118 0.39212 0.031856 -0.2494 0.025445 0.023539 0.51135 -0.04833 0.075799 0.23541 0.14268 -0.14174 -0.19002 0.13089 0.034636 0.0020952 -0.07869 0.27146 0.26171 0.25223 -0.54038 0.21883 0.33038 0.078631 -0.35918 -0.67294 0.20977 -0.71348 0.0086549 -0.12238 0.40599 0.018263 0.60361 0.12501 0.17804 0.1642 -0.38815 0.33492 -0.032672 -0.0035534 0.082885 0.046923 0.30388 -0.034793 -0.18302 0.42393 -0.33337 0.53172 -0.23185 0.041988 0.2248 -0.064892 -0.091652 0.53017 -0.21532 0.23638 0.25911 0.1584 0.27418 0.35827 -0.039986 0.2424 0.25426 0.23064 0.11992 -0.10334 0.061444 0.42895 -0.13696 -0.69976 -0.037393 -0.30263 -0.08402 -0.12771 -0.28188 0.23552 -0.28959 -5.7042 0.55896 -0.066328 0.21431 -0.059026 -0.028914 0.25343 0.52091 0.13443 -0.17507 -0.51028 -0.16934 -0.2233 -0.14 -0.42088 0.044559 0.53796 0.23539 0.0714 0.017734 0.13151 -0.28951 -0.23843 -0.39196 -0.29446 -0.45591 -0.31632 -0.3744 0.068207 -0.069802 0.21083 -0.27317 0.22575 -0.60759 0.19857 0.67895 0.13279 0.12491 -0.49658 0.40854 0.074004 -0.30572 -0.37961 0.11959 0.05845 0.30813 -0.45748 0.19981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(lines[200])\n",
    "    print(lines[201])\n",
    "    print(lines[202])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data structures\n",
    "\n",
    "We will create a few data structures that will allow us to easily reference parts of the dataset later on\n",
    "\n",
    "1. The list `wordList` will store the words indexed by the order in which the words appear in the input file\n",
    "\n",
    "2. The matrix `wordVectorMatrix` will store the word vectors indexed by the order of `wordList`\n",
    "\n",
    "3. The dictionary `wordVectorDictionary` will store the (word, word vector) pairs\n",
    "\n",
    "More information about using defaultdict can be found [here](http://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work) and [here](http://stackoverflow.com/questions/19629682/ordereddict-vs-defaultdict-vs-dict)\n",
    "\n",
    "We will limit the number of words to read in here because of the size of the dataset. The variable `readWords` will hold the indices we will read in. Let's try read in the first 100,000 words, which takes about 3 minutes on my computer. If you want to read in a different subset, or the entire dataset, simply update the `readWords` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readWords = range(100000)\n",
    "\n",
    "wordList = []\n",
    "wordVectorMatrix = np.zeros((len(readWords),numDimensions))\n",
    "wordVectorDictionary = collections.defaultdict(list)\n",
    "\n",
    "with open(filename,'r') as f:\n",
    "    index = 0\n",
    "    for line in f:\n",
    "        if index in readWords:\n",
    "            split = line.split()\n",
    "            word = (split[0])\n",
    "            wordList.append(word)\n",
    "            listValues = map(float, split[1:])\n",
    "            wordVectorMatrix[index] = listValues\n",
    "            wordVectorDictionary[word] = listValues\n",
    "            index += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have successfully read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of wordList is: 100000, and the length of wordVectorDictionary is: 100000\n",
      "The dimensions of wordVectorMatrix are: (100000, 200)\n",
      "The 100000-th word in our word list is: валентина\n",
      "The first 5 dimensions of \"валентина\" are: (-0.13675999999999999, -0.43297999999999998, -0.47603000000000001, -0.48025000000000001, -0.46645999999999999)\n",
      "Does the vector in wordVectorDictionary for \"валентина\" match the vector in wordVectorMatrix: True\n"
     ]
    }
   ],
   "source": [
    "print('The length of wordList is: %d, and the length of wordVectorDictionary is: %d' %(len(wordList),len(wordVectorDictionary))) \n",
    "print('The dimensions of wordVectorMatrix are: %s' %(wordVectorMatrix.shape,))\n",
    "\n",
    "print('The %d-th word in our word list is: %s' %(len(wordList),wordList[-1]))\n",
    "print('The first 5 dimensions of \"%s\" are: %s' %(wordList[-1], tuple(wordVectorMatrix[-1,:5])))\n",
    "print('Does the vector in wordVectorDictionary for \"%s\" match the vector in wordVectorMatrix: %s' %(wordList[-1],all(wordVectorDictionary[wordList[-1]] == wordVectorMatrix[-1,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary. For example, here are the closest words to the word \"please\" from our `wordList` vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findClosestWords(word, numWords):\n",
    "    indexOfWord = wordList.index(word)\n",
    "    wordVector = wordVectorMatrix[indexOfWord]\n",
    "    similarityDictionary = {}\n",
    "    for i in readWords:\n",
    "        if i == indexOfWord:\n",
    "            continue\n",
    "        closeness = 1 - spatial.distance.cosine(wordVector, wordVectorMatrix[i,:])\n",
    "        similarityDictionary[wordList[i]] = closeness\n",
    "    for w in sorted(similarityDictionary, key=similarityDictionary.get, reverse=True)[:numWords]:\n",
    "        print(w, similarityDictionary[w]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pls', 0.86398293210897281)\n",
      "('plz', 0.78704690310582259)\n",
      "('help', 0.78122570098971478)\n",
      "('pleasee', 0.77198784350021787)\n",
      "('you', 0.73852868220176038)\n",
      "('follow', 0.72495757262223237)\n",
      "('need', 0.71108471050433231)\n",
      "('guys', 0.70307052290636773)\n",
      "('can', 0.70267652777623324)\n",
      "('if', 0.70099452649437954)\n"
     ]
    }
   ],
   "source": [
    "closestWordList = findClosestWords('please', 10)\n",
    "closestWordList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

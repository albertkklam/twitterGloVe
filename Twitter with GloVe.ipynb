{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe introduction\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space\n",
    "\n",
    "More information can be found here: http://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We will use word vectors that have been trained on the Twitter dataset (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors). This file can be downloaded from the GloVe website. Extract the .txt file and move it to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset\n",
    "\n",
    "Let's read in the data and do some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193514, 200)\n"
     ]
    }
   ],
   "source": [
    "with open(filename,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    numWords = len(lines)\n",
    "    numDimensions = len(lines[200].split(' ')[1:])\n",
    "    print(numWords, numDimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's slightly less words in the vocabulary than claimed (1,193,514 words vs. 1,200,000) and each word is mapped to a 200 dimensional vector. Let's have a look at what a word vector looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please 0.079204 0.38973 -0.15059 -0.010345 -0.43449 -1.0396 1.142 -0.12891 0.021345 -0.31301 0.67416 0.020708 -0.21758 -0.25822 -0.087623 -0.21197 0.19887 -0.18434 0.11543 -0.045039 -0.21852 -0.4629 -0.40147 0.88832 -0.28331 0.15793 0.43682 0.62241 0.29734 0.025521 0.04076 0.42191 -0.17571 0.38485 -0.222 -0.12087 0.53335 0.60102 -0.14619 -0.2134 0.33717 -0.46093 -0.31229 0.0040756 -0.11045 -0.26965 -0.64615 -0.66332 0.39245 0.10454 0.073493 0.54851 0.36091 -1.1031 0.25083 0.06513 0.046064 0.56705 -0.072345 -0.19426 0.17681 -0.13486 0.33334 -0.18167 0.11279 0.42252 -0.11612 -0.10706 0.1187 -0.044723 0.053748 0.064657 0.12535 0.04816 -0.29935 -0.10651 -0.29289 -0.18884 -0.4127 0.32664 -0.22715 0.67269 0.41074 -0.35499 0.38288 0.083866 -0.76714 0.29737 -0.27832 0.2076 0.015894 -0.074241 0.040225 0.46588 -0.73723 -0.18881 0.062039 0.27367 0.12206 -0.3957 -0.21934 -0.065962 -0.19748 -0.18956 0.027889 -0.126 0.037872 0.54629 0.37619 -0.26709 -0.27878 -0.12053 -0.66942 0.064615 1.2675 0.057054 -0.27236 -0.095923 0.37097 0.0404 -0.48124 0.068381 0.11831 -0.029161 0.69429 -0.15623 -0.51732 -0.10252 -0.029082 1.1329 0.25229 -0.093178 0.59033 -0.15265 0.59541 -0.046142 0.31834 0.52373 0.041039 -0.1108 -0.13369 -0.23856 -0.046343 0.12303 -0.40135 0.11632 0.2686 0.2048 -0.2025 -0.29742 -0.10665 -0.0082937 -5.2843 0.58139 -0.49934 0.21979 -0.48109 -0.34275 0.22535 0.27961 0.57855 -0.86456 -0.11685 -0.051452 -0.12701 -0.44353 0.49309 0.24619 -0.49139 0.58254 0.15034 -0.47087 0.20892 -0.6286 -0.27853 -0.43544 0.19348 -0.29692 -0.34797 -0.53869 0.22205 -0.52296 0.55458 0.43955 0.013287 -0.73236 -0.52186 0.070435 0.24036 0.18332 0.035254 0.31854 -0.25702 -0.26639 -0.39825 -0.57111 -0.34293 -0.16897 0.4296 0.022703\n",
      "\n",
      "via -0.081437 0.069476 0.23265 -0.40076 -0.19034 -0.49862 -0.73485 -0.58641 0.54637 0.33817 0.10446 0.2993 -0.50904 0.15863 -0.1678 -0.11511 -0.54687 -0.085679 0.60643 -0.16164 -0.27374 -0.32401 -0.52601 -0.23146 0.059834 -0.52935 -0.42386 0.098854 -0.18035 0.13567 0.15574 -0.29407 -0.033457 -0.32343 1.0269 -0.2487 0.27185 0.0097102 -0.13825 -0.13629 -1.1641 0.38467 -0.15126 0.0025759 0.19337 -0.093839 -0.25593 -0.051334 -0.58863 -0.58854 0.23793 -0.58243 0.042192 0.60617 0.29011 0.081879 0.53839 -0.37073 0.37057 0.1838 -0.01204 -0.11125 0.19795 0.74184 0.011207 -0.16587 0.35526 0.10512 0.18121 0.53045 0.47704 -0.10744 0.53014 -0.1772 -0.61374 -0.25991 -0.50574 -0.16052 -0.25734 0.03218 0.082489 0.53998 1.1106 -0.21224 0.37242 -0.89264 0.31962 0.40549 0.21223 -0.25742 -0.087136 -0.28339 0.25084 -0.092796 -0.42794 0.2892 -0.14682 0.16072 -0.31523 -0.14671 0.4218 0.52531 0.46894 0.039444 0.43839 0.43707 0.027612 -0.057925 -0.030428 -0.51068 0.42887 0.37742 -0.21719 0.63096 0.70359 0.56866 -0.10866 -0.71073 -0.29489 0.46609 0.58452 0.26705 0.242 -0.54905 -0.097825 -0.31057 -0.5359 0.45974 0.27342 0.49491 0.46291 0.24952 -0.26941 0.19397 0.46285 0.16044 0.093137 0.55253 0.14519 -0.24617 0.14566 -0.17531 -0.079643 -0.21696 0.14657 0.071494 -0.39468 -0.18362 -0.0038123 0.45462 -0.44943 -0.023135 -4.0527 0.12995 0.43728 -0.51105 0.14058 0.14181 0.0027643 0.3928 -0.17767 0.36714 0.26078 -0.0068611 0.49727 -0.56449 -0.33131 0.075125 -0.61215 0.19659 0.30055 0.49532 0.21603 0.16426 -0.064625 0.26461 0.75414 -0.22029 -0.10808 0.3196 0.36554 -0.27764 -0.083928 0.20858 -0.33401 -0.138 -0.18531 -0.4884 0.35663 0.088559 0.09485 -0.10018 0.1194 0.19082 0.1525 0.78007 -0.3482 0.2529 -0.27805 0.32401\n",
      "\n",
      "much -0.22675 0.72306 0.030062 0.56328 -0.21531 0.23392 0.70151 0.067725 -0.46529 -0.45717 0.16344 -0.25265 -0.87955 -0.16559 -0.067855 0.11523 0.11271 0.11077 -0.42187 -0.16591 0.066559 -0.40993 0.51975 0.36871 0.055161 1.2822 0.19334 0.51311 0.32885 -0.20996 -0.12559 0.0090897 0.25718 0.41892 -0.27265 -0.02081 -0.23258 0.17777 -0.51448 0.12906 0.49615 -0.075301 0.30338 -0.34572 -0.13291 -0.19647 0.33912 -0.49893 0.10305 0.019522 0.15939 -0.087156 0.047154 -0.30672 -0.09427 0.025437 0.094385 0.18369 -0.45566 -0.28899 -0.54054 -0.45882 -0.61685 -0.2597 -0.0030352 0.13973 -0.54018 -0.68448 0.042326 -0.22023 -0.051847 -0.23852 0.083819 0.08203 -0.1727 -0.037118 0.39212 0.031856 -0.2494 0.025445 0.023539 0.51135 -0.04833 0.075799 0.23541 0.14268 -0.14174 -0.19002 0.13089 0.034636 0.0020952 -0.07869 0.27146 0.26171 0.25223 -0.54038 0.21883 0.33038 0.078631 -0.35918 -0.67294 0.20977 -0.71348 0.0086549 -0.12238 0.40599 0.018263 0.60361 0.12501 0.17804 0.1642 -0.38815 0.33492 -0.032672 -0.0035534 0.082885 0.046923 0.30388 -0.034793 -0.18302 0.42393 -0.33337 0.53172 -0.23185 0.041988 0.2248 -0.064892 -0.091652 0.53017 -0.21532 0.23638 0.25911 0.1584 0.27418 0.35827 -0.039986 0.2424 0.25426 0.23064 0.11992 -0.10334 0.061444 0.42895 -0.13696 -0.69976 -0.037393 -0.30263 -0.08402 -0.12771 -0.28188 0.23552 -0.28959 -5.7042 0.55896 -0.066328 0.21431 -0.059026 -0.028914 0.25343 0.52091 0.13443 -0.17507 -0.51028 -0.16934 -0.2233 -0.14 -0.42088 0.044559 0.53796 0.23539 0.0714 0.017734 0.13151 -0.28951 -0.23843 -0.39196 -0.29446 -0.45591 -0.31632 -0.3744 0.068207 -0.069802 0.21083 -0.27317 0.22575 -0.60759 0.19857 0.67895 0.13279 0.12491 -0.49658 0.40854 0.074004 -0.30572 -0.37961 0.11959 0.05845 0.30813 -0.45748 0.19981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(lines[200])\n",
    "    print(lines[201])\n",
    "    print(lines[202])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data structures\n",
    "\n",
    "We will create a few data structures that will allow us to easily reference parts of the dataset later on\n",
    "\n",
    "1. The list `wordList` will store the words indexed by the order in which the words appear in the input file\n",
    "\n",
    "2. The matrix `wordVectorMatrix` will store the word vectors indexed by the order of `wordList`\n",
    "\n",
    "3. The dictionary `wordVectorDictionary` will store the (word, word vector) pairs\n",
    "\n",
    "More information about using defaultdict can be found [here](http://stackoverflow.com/questions/5900578/how-does-collections-defaultdict-work) and [here](http://stackoverflow.com/questions/19629682/ordereddict-vs-defaultdict-vs-dict)\n",
    "\n",
    "We will limit the number of words to read in here because of the size of the dataset. The variable `readWords` will hold the indices we will read in. Let's try read in the first 100,000 words, which takes about 3 minutes on my computer. If you want to read in a different subset, or the entire dataset, simply update the `readWords` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readWords = range(100000)\n",
    "\n",
    "wordList = []\n",
    "wordVectorMatrix = np.zeros((len(readWords),numDimensions))\n",
    "wordVectorDictionary = collections.defaultdict(list)\n",
    "\n",
    "with open(filename,'r') as f:\n",
    "    index = 0\n",
    "    for line in f:\n",
    "        if index in readWords:\n",
    "            split = line.split()\n",
    "            word = (split[0])\n",
    "            wordList.append(word)\n",
    "            listValues = map(float, split[1:])\n",
    "            wordVectorMatrix[index] = listValues\n",
    "            wordVectorDictionary[word] = listValues\n",
    "            index += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have successfully read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of wordList is: 100000, and the length of wordVectorDictionary is: 100000\n",
      "The dimensions of wordVectorMatrix are: (100000, 200)\n",
      "The 100000-th word in our word list is: валентина\n",
      "The first 5 dimensions of \"валентина\" are: (-0.13675999999999999, -0.43297999999999998, -0.47603000000000001, -0.48025000000000001, -0.46645999999999999)\n",
      "Does the vector in wordVectorDictionary for \"валентина\" match the vector in wordVectorMatrix: True\n"
     ]
    }
   ],
   "source": [
    "print('The length of wordList is: %d, and the length of wordVectorDictionary is: %d' %(len(wordList),len(wordVectorDictionary))) \n",
    "print('The dimensions of wordVectorMatrix are: %s' %(wordVectorMatrix.shape,))\n",
    "\n",
    "print('The %d-th word in our word list is: %s' %(len(wordList),wordList[-1]))\n",
    "print('The first 5 dimensions of \"%s\" are: %s' %(wordList[-1], tuple(wordVectorMatrix[-1,:5])))\n",
    "print('Does the vector in wordVectorDictionary for \"%s\" match the vector in wordVectorMatrix: %s' %(wordList[-1],all(wordVectorDictionary[wordList[-1]] == wordVectorMatrix[-1,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "(From http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary. For example, here are the closest words to the word \"please\" from our `wordList` vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findClosestWords(word, numWords):\n",
    "    indexOfWord = wordList.index(word)\n",
    "    wordVector = wordVectorMatrix[indexOfWord]\n",
    "    similarityDictionary = {}\n",
    "    for i in readWords:\n",
    "        if i == indexOfWord:\n",
    "            continue\n",
    "        closeness = 1 - spatial.distance.cosine(wordVector, wordVectorMatrix[i,:])\n",
    "        similarityDictionary[wordList[i]] = closeness\n",
    "    for w in sorted(similarityDictionary, key=similarityDictionary.get, reverse=True)[:numWords]:\n",
    "        print(w, similarityDictionary[w]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pls', 0.86398293210897281)\n",
      "('plz', 0.78704690310582259)\n",
      "('help', 0.78122570098971478)\n",
      "('pleasee', 0.77198784350021787)\n",
      "('you', 0.73852868220176038)\n",
      "('follow', 0.72495757262223237)\n",
      "('need', 0.71108471050433231)\n",
      "('guys', 0.70307052290636773)\n",
      "('can', 0.70267652777623324)\n",
      "('if', 0.70099452649437954)\n"
     ]
    }
   ],
   "source": [
    "findClosestWords('please', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear substructures\n",
    "\n",
    "(From http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, **`man`** may be regarded as similar to **`woman`** in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.\n",
    "\n",
    "In order to capture in a quantitative way the nuance necessary to distinguish **`man`** from **`woman`**, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.\n",
    "\n",
    "The underlying concept that distinguishes **`man`** from **`woman`**, i.e. sex or gender, may be equivalently specified by various other word pairs, such as **`king`** and **`queen`** or **`brother`** and **`sister`**. To state this observation mathematically, we might expect that the vector differences **`man`** - **`woman`**, **`king`** - **`queen`**, and **`brother`** - **`sister`** might all be roughly equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a simple helper function `subtractVectors` to calculate the difference between our word vectors. We can then measure the similarity between the differenced vectors - here, we will use the 2-norm. As a toy baseline, we also difference the **`man`** and **`king`** word vectors and compare it to one of our target vectors. We would expect the difference in **`man`** and **`king`** to be captured in some \"class / royalty\" dimension(s), while differences in our target vectors would be captured in some \"sex / gender\" dimension(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def subtractVectors(v1, v2):\n",
    "    return np.subtract(wordVectorDictionary[v1], wordVectorDictionary[v2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.78044228646\n",
      "5.06755389764\n",
      "3.19655465982\n",
      "6.66202547325\n",
      "\n",
      "\n",
      "6.10387516356\n",
      "5.18603536938\n",
      "4.42713856161\n",
      "\n",
      "\n",
      "7.10890911683\n",
      "9.08774112031\n",
      "7.10114498388\n"
     ]
    }
   ],
   "source": [
    "manWoman = subtractVectors('man','woman')\n",
    "kingQueen = subtractVectors('king','queen')\n",
    "brotherSister = subtractVectors('brother','sister')\n",
    "manKing = subtractVectors('man','king')\n",
    "\n",
    "print(np.linalg.norm(manWoman, 2))\n",
    "print(np.linalg.norm(kingQueen, 2))\n",
    "print(np.linalg.norm(brotherSister, 2))\n",
    "print(np.linalg.norm(manKing, 2))\n",
    "print(\"\\n\")\n",
    "print(np.linalg.norm(manWoman - kingQueen, 2))\n",
    "print(np.linalg.norm(manWoman - brotherSister, 2))\n",
    "print(np.linalg.norm(kingQueen - brotherSister, 2))\n",
    "print(\"\\n\")\n",
    "print(np.linalg.norm(manWoman - manKing, 2))\n",
    "print(np.linalg.norm(kingQueen - manKing, 2))\n",
    "print(np.linalg.norm(brotherSister - manKing, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like there is a very clear differentiation here - our differenced target vectors are more or less as similar as our differenced baseline toy vector. Let's have a look at the closest words of our target words to see what could be happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dude', 0.72534557547796541)\n",
      "('boy', 0.71745345199436339)\n",
      "('guy', 0.69980123081425893)\n",
      "('shit', 0.6852256903999735)\n",
      "('was', 0.6779443289268865)\n",
      "(\"'s\", 0.67626148952588672)\n",
      "('bad', 0.67340113492396148)\n",
      "('men', 0.67148744679465711)\n",
      "('that', 0.6709873650567636)\n",
      "('lol', 0.66466723938892469)\n",
      "\n",
      "\n",
      "('girl', 0.78170500548436095)\n",
      "('women', 0.77058475822255024)\n",
      "('guy', 0.71543143302767331)\n",
      "('she', 0.71043631187997947)\n",
      "('person', 0.70346465646181122)\n",
      "('wife', 0.70295824095398418)\n",
      "('female', 0.70005288497867146)\n",
      "('mother', 0.69949987619893406)\n",
      "('lady', 0.69457613712699473)\n",
      "('who', 0.67055183627737946)\n",
      "\n",
      "\n",
      "('prince', 0.71705487042753235)\n",
      "('queen', 0.68892980731736941)\n",
      "('aka', 0.64715133442100214)\n",
      "('kings', 0.61939561453656489)\n",
      "('burger', 0.60857509787113406)\n",
      "('the', 0.60779876557683776)\n",
      "(\"'s\", 0.59723619899538138)\n",
      "('legend', 0.592567479652929)\n",
      "('john', 0.58624949677325577)\n",
      "('jack', 0.58563818606440887)\n",
      "\n",
      "\n",
      "('princess', 0.7377367648175821)\n",
      "('king', 0.68892980731736941)\n",
      "('queens', 0.63387349504555346)\n",
      "('diva', 0.62183066711201751)\n",
      "('lady', 0.61800048896655435)\n",
      "('gaga', 0.59439640065768518)\n",
      "('she', 0.59138934085054651)\n",
      "('prince', 0.59060030541968223)\n",
      "('girl', 0.58766604145773893)\n",
      "('beyonce', 0.58212593209138641)\n",
      "\n",
      "\n",
      "('sister', 0.87537393350271109)\n",
      "('dad', 0.84109701434617035)\n",
      "('cousin', 0.81943265462658965)\n",
      "('friend', 0.78122012013751996)\n",
      "('mom', 0.76149448129288455)\n",
      "('uncle', 0.75296123254839997)\n",
      "('dude', 0.74476784774024218)\n",
      "('nephew', 0.73669038379146423)\n",
      "('brothers', 0.7351889717810155)\n",
      "('father', 0.72951861101950422)\n",
      "\n",
      "\n",
      "('brother', 0.87537393350271109)\n",
      "('cousin', 0.86010426323590006)\n",
      "('mom', 0.8553427452126362)\n",
      "('dad', 0.83729066009133157)\n",
      "('sisters', 0.83569539592363029)\n",
      "('friend', 0.82435563747290141)\n",
      "('niece', 0.79951804822272587)\n",
      "('daughter', 0.78435997007908964)\n",
      "('aunt', 0.77375989736507411)\n",
      "('grandma', 0.77280648892821158)\n"
     ]
    }
   ],
   "source": [
    "findClosestWords('man', 10)\n",
    "print(\"\\n\")\n",
    "findClosestWords('woman', 10)\n",
    "print(\"\\n\")\n",
    "findClosestWords('king', 10)\n",
    "print(\"\\n\")\n",
    "findClosestWords('queen', 10)\n",
    "print(\"\\n\")\n",
    "findClosestWords('brother', 10)\n",
    "print(\"\\n\")\n",
    "findClosestWords('sister', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few things here:\n",
    "\n",
    "1. As expected, **`man`** and **`woman`** have quite distinct neighbours. **`guy`** is a common neighbour to both, possibly because of colloquial phrases like \"hey guys\" that references both genders. We can confirm this by looking at the closest neighbours to **`guy`**\n",
    "2. There's some level of noise in the closest neighbours of **`man`** that we would not normally associate with the word. This is most likely borne from the way Twitter is used, and could be indicative of poor vector representation \n",
    "3. **`king`** and **`queen`** are the second closest neighbours to each other. Again, some level of noise in both of these neighbourhoods\n",
    "4. **`brother`** and **`sister`** are the closest neighbours to each other, but all other neighbours are plausible. These two vectors are most likely capturing the meaning of their target words the best\n",
    "\n",
    "Overall, there is some deviation in how Twitter is using some of the most common words in the English language, and this is most likely feeding into some of the unexpected results. Words which are less frequently used in a colloquial sense tend to align with expectations the most"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
